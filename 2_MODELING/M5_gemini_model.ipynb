{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1437,
     "status": "ok",
     "timestamp": 1764950364699,
     "user": {
      "displayName": "Juan José Corredor Ojeda",
      "userId": "03283161025982388119"
     },
     "user_tz": 300
    },
    "id": "Pjy_3DHvde9E",
    "outputId": "9e5ae80f-3c4b-4cd8-bb1e-32f7bc9d2092"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import ast\n",
    "import gc\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Gemini client\n",
    "from google import genai\n",
    "from google.genai import types as genai_types\n",
    "\n",
    "load_dotenv(\"../.env.prod\")\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "API_KEY = os.getenv(\"GEMINI_KEY\")\n",
    "client = genai.Client(api_key=\"API_KEY\")\n",
    "\n",
    "GEMINI_EMBEDDING_MODEL = \"gemini-embedding-001\"\n",
    "GEMINI_OUTPUT_DIM = 768\n",
    "\n",
    "\n",
    "\n",
    "BATCH_GEMINI = 64\n",
    "BATCH_TRAIN = 512\n",
    "LR = 2e-4\n",
    "N_EPOCHS = 2\n",
    "LOC_OUT_DIM = 32\n",
    "PROJ_DIM = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 763213,
     "status": "ok",
     "timestamp": 1764951138683,
     "user": {
      "displayName": "Juan José Corredor Ojeda",
      "userId": "03283161025982388119"
     },
     "user_tz": 300
    },
    "id": "OORoIsxrdOnf",
    "outputId": "4dbfbfd4-c656-4893-96f1-1bf1e11e0891"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "Fourier dim: 8\n",
      "Train rows: 2000  Val rows: 2000\n",
      "\n",
      "Precomputing Gemini embeddings for train+val texts...\n",
      "Unique truncated texts to embed with Gemini: 10021\n",
      "[Gemini] Rate limit reached, sleeping 0.5s...\n",
      "[Gemini] Rate limit reached, sleeping 0.5s...\n",
      "[Gemini] Rate limit reached, sleeping 0.5s...\n",
      "[Gemini] Rate limit reached, sleeping 0.5s...\n",
      "[Gemini] Rate limit reached, sleeping 0.5s...\n",
      "[Gemini] Rate limit reached, sleeping 0.5s...\n",
      "[Gemini] Rate limit reached, sleeping 0.5s...\n",
      "[Gemini] Rate limit reached, sleeping 0.5s...\n",
      "[Gemini] Rate limit reached, sleeping 0.5s...\n",
      "[Gemini] Rate limit reached, sleeping 0.5s...\n",
      "[Gemini] Rate limit reached, sleeping 0.5s...\n",
      "[Gemini] Rate limit reached, sleeping 0.5s...\n",
      "Total original texts mapped: 10029\n",
      "Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def normalize_text(x: str) -> str:\n",
    "    if not isinstance(x, str):\n",
    "        x = \"\" if x is None else str(x)\n",
    "    return x[:MAX_CHARS_PER_TEXT]\n",
    "\n",
    "def to_1d_float_col(col: pd.Series) -> pd.Series:\n",
    "    def fix(x):\n",
    "        if x is None:\n",
    "            return []\n",
    "        if isinstance(x, str):\n",
    "            x = ast.literal_eval(x)\n",
    "        return [float(t) for t in x]\n",
    "    return col.apply(fix)\n",
    "\n",
    "\n",
    "base_dir = \"/content/drive/MyDrive/thesis/2_MODELING\"\n",
    "\n",
    "train = pd.read_parquet(\"../files/processed/paired_datasets/train.parquet\").sample(10000)\n",
    "val   = pd.read_parquet(\"../files/processed/paired_datasets/val.parquet\").sample(10000)\n",
    "\n",
    "keep_cols = [\n",
    "    \"vacant_id\",\n",
    "    \"pos_candidate_id\",\n",
    "    \"neg_candidate_id\",\n",
    "    \"pos_candidate_full_text\",\n",
    "    \"neg_candidate_full_text\",\n",
    "    \"neg_vacant_full_text\",\n",
    "    \"neg_vacant_fourier_feature\",\n",
    "    \"pos_candidate_fourier_features\",\n",
    "    \"neg_candidate_fourier_features\",\n",
    "]\n",
    "\n",
    "train = train[keep_cols].sample(10000) # limitamos las filas por el costo del API\n",
    "val   = val[keep_cols].sample(10000)\n",
    "\n",
    "train = train.rename(\n",
    "    columns={\n",
    "        \"neg_vacant_full_text\": \"anchor_full_text\",\n",
    "        \"neg_vacant_fourier_feature\": \"anchor_fourier_feature\",\n",
    "    }\n",
    ")\n",
    "val = val.rename(\n",
    "    columns={\n",
    "        \"neg_vacant_full_text\": \"anchor_full_text\",\n",
    "        \"neg_vacant_fourier_feature\": \"anchor_fourier_feature\",\n",
    "    }\n",
    ")\n",
    "\n",
    "if TRAIN_FRAC < 1.0:\n",
    "    train = train.sample(frac=TRAIN_FRAC, random_state=42)\n",
    "if VAL_FRAC < 1.0:\n",
    "    val = val.sample(frac=VAL_FRAC, random_state=42)\n",
    "\n",
    "train[\"anchor_fourier_feature\"] = to_1d_float_col(train[\"anchor_fourier_feature\"])\n",
    "train[\"pos_candidate_fourier_features\"] = to_1d_float_col(train[\"pos_candidate_fourier_features\"])\n",
    "train[\"neg_candidate_fourier_features\"] = to_1d_float_col(train[\"neg_candidate_fourier_features\"])\n",
    "\n",
    "val[\"anchor_fourier_feature\"] = to_1d_float_col(val[\"anchor_fourier_feature\"])\n",
    "val[\"pos_candidate_fourier_features\"] = to_1d_float_col(val[\"pos_candidate_fourier_features\"])\n",
    "val[\"neg_candidate_fourier_features\"] = to_1d_float_col(val[\"neg_candidate_fourier_features\"])\n",
    "\n",
    "example_fourier = np.array(train[\"anchor_fourier_feature\"].iloc[0], dtype=\"float32\")\n",
    "fourier_dim = example_fourier.shape[-1]\n",
    "print(\"Fourier dim:\", fourier_dim)\n",
    "print(\"Train rows:\", len(train), \" Val rows:\", len(val))\n",
    "\n",
    "\n",
    "import time\n",
    "from typing import Dict, List\n",
    "\n",
    "MAX_CHARS_PER_TEXT = 2000          # truncate long descriptions\n",
    "MAX_TOTAL_TEXTS    = None          # e.g. 5000, or None for unlimited\n",
    "MAX_REQUESTS_PER_MIN = 100          # <-- set this to your Gemini tier limit\n",
    "GEMINI_BATCH_SIZE  = 8             # smaller batch size = fewer tokens per call\n",
    "\n",
    "\n",
    "def encode_gemini(texts: List[str],\n",
    "                  batch_size: int = GEMINI_BATCH_SIZE) -> np.ndarray:\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    if not texts:\n",
    "        return np.zeros((0, GEMINI_OUTPUT_DIM), dtype=\"float32\")\n",
    "\n",
    "    all_vecs = []\n",
    "\n",
    "    cfg = genai_types.EmbedContentConfig(\n",
    "        task_type=\"SEMANTIC_SIMILARITY\", # escogemos la especialidad de búsqueda semántica como se explica en el texto\n",
    "        output_dimensionality=GEMINI_OUTPUT_DIM,\n",
    "    )\n",
    "\n",
    "    window_start = time.time()\n",
    "    calls_in_window = 0\n",
    "    min_secs_per_call = 60.0 / MAX_REQUESTS_PER_MIN if MAX_REQUESTS_PER_MIN else 0.0\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "\n",
    "        if MAX_REQUESTS_PER_MIN:\n",
    "            now = time.time()\n",
    "            if now - window_start >= 60.0:\n",
    "                window_start = now\n",
    "                calls_in_window = 0\n",
    "\n",
    "            if calls_in_window >= MAX_REQUESTS_PER_MIN:\n",
    "                sleep_for = 60.0 - (now - window_start)\n",
    "                if sleep_for > 0:\n",
    "                    print(f\"[Gemini] Rate limit reached, sleeping {sleep_for:.1f}s...\")\n",
    "                    time.sleep(sleep_for)\n",
    "                window_start = time.time()\n",
    "                calls_in_window = 0\n",
    "\n",
    "            if calls_in_window > 0 and min_secs_per_call > 0:\n",
    "                elapsed = now - window_start\n",
    "                expected_time = calls_in_window * min_secs_per_call\n",
    "                if elapsed < expected_time:\n",
    "                    sleep_for = expected_time - elapsed\n",
    "                    time.sleep(sleep_for)\n",
    "\n",
    "        result = client.models.embed_content(\n",
    "            model=GEMINI_EMBEDDING_MODEL,\n",
    "            contents=batch_texts,\n",
    "            config=cfg,\n",
    "        )\n",
    "        calls_in_window += 1\n",
    "\n",
    "        batch_vecs = [np.asarray(e.values, dtype=\"float32\") for e in result.embeddings]\n",
    "        batch_vecs = np.stack(batch_vecs, axis=0)\n",
    "        all_vecs.append(batch_vecs)\n",
    "\n",
    "    arr = np.concatenate(all_vecs, axis=0)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def build_text2emb_mapping(dfs: List[pd.DataFrame]) -> Dict[str, np.ndarray]:\n",
    "\n",
    "    original_texts: List[str] = []\n",
    "    for df in dfs:\n",
    "        original_texts.extend(df[\"anchor_full_text\"].tolist())\n",
    "        original_texts.extend(df[\"pos_candidate_full_text\"].tolist())\n",
    "        original_texts.extend(df[\"neg_candidate_full_text\"].tolist())\n",
    "\n",
    "    orig_to_trunc: Dict[str, str] = {}\n",
    "    for t in original_texts:\n",
    "        t_orig = \"\" if t is None else str(t)\n",
    "        t_trunc = normalize_text(t_orig)\n",
    "        orig_to_trunc[t_orig] = t_trunc\n",
    "\n",
    "    truncated_texts = list(dict.fromkeys(orig_to_trunc.values()))\n",
    "    print(\"Unique truncated texts to embed with Gemini:\", len(truncated_texts))\n",
    "\n",
    "    embs = encode_gemini(truncated_texts)\n",
    "    trunc2emb = {t: emb for t, emb in zip(truncated_texts, embs)}\n",
    "\n",
    "    text2emb: Dict[str, np.ndarray] = {}\n",
    "    for t_orig, t_trunc in orig_to_trunc.items():\n",
    "        text2emb[t_orig] = trunc2emb[t_trunc]\n",
    "\n",
    "    print(\"Total original texts mapped:\", len(text2emb))\n",
    "    return text2emb\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nPrecomputing Gemini embeddings for train+val texts...\")\n",
    "text2emb = build_text2emb_mapping([train, val])\n",
    "print(\"Done.\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6gqXe8CPe68I"
   },
   "outputs": [],
   "source": [
    "\n",
    "class GeminiFourierTripletDataset(Dataset): # M5 previo\n",
    "    def __init__(self, df: pd.DataFrame, text2emb: Dict[str, np.ndarray]):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.text2emb = text2emb\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        a_text = self.text2emb[row[\"anchor_full_text\"]]\n",
    "        p_text = self.text2emb[row[\"pos_candidate_full_text\"]]\n",
    "        n_text = self.text2emb[row[\"neg_candidate_full_text\"]]\n",
    "\n",
    "        a_four = np.asarray(row[\"anchor_fourier_feature\"], dtype=\"float32\")\n",
    "        p_four = np.asarray(row[\"pos_candidate_fourier_features\"], dtype=\"float32\")\n",
    "        n_four = np.asarray(row[\"neg_candidate_fourier_features\"], dtype=\"float32\")\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(a_text),\n",
    "            torch.from_numpy(p_text),\n",
    "            torch.from_numpy(n_text),\n",
    "            torch.from_numpy(a_four),\n",
    "            torch.from_numpy(p_four),\n",
    "            torch.from_numpy(n_four),\n",
    "        )\n",
    "\n",
    "\n",
    "train_dataset = GeminiFourierTripletDataset(train, text2emb)\n",
    "val_dataset   = GeminiFourierTripletDataset(val, text2emb)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_TRAIN,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_TRAIN,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SbJqfRf-fVYx"
   },
   "outputs": [],
   "source": [
    "\n",
    "class LocHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple MLP for location Fourier features -> loc_out_dim.\n",
    "    If you already have a LocHead implementation in loc_mpnet.Model, you can\n",
    "    swap this class for that one to keep it 100% identical.\n",
    "    \"\"\"\n",
    "    def __init__(self, fourier_dim: int, loc_out_dim: int = LOC_OUT_DIM):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(fourier_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, loc_out_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class GeminiHead(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_dim: int,\n",
    "        fourier_dim: int,\n",
    "        proj_dim: int = PROJ_DIM,\n",
    "        loc_out_dim: int = LOC_OUT_DIM,\n",
    "        use_location: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.use_location = use_location\n",
    "\n",
    "        if use_location:\n",
    "            self.loc_head = LocHead(fourier_dim, loc_out_dim)\n",
    "            in_dim = text_dim + loc_out_dim\n",
    "        else:\n",
    "            self.loc_head = None\n",
    "            in_dim = text_dim\n",
    "\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(in_dim, proj_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(proj_dim, proj_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, text_emb: torch.Tensor, fourier: torch.Tensor | None) -> torch.Tensor:\n",
    "        # Normalize Gemini text embedding first\n",
    "        x = F.normalize(text_emb, p=2, dim=-1)\n",
    "\n",
    "        if self.use_location:\n",
    "            loc = self.loc_head(fourier)\n",
    "            x = torch.cat([x, loc], dim=-1)\n",
    "\n",
    "        x = self.proj(x)\n",
    "        x = F.normalize(x, p=2, dim=-1)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Two heads that we will train separately:\n",
    "head_text_only = GeminiHead(\n",
    "    text_dim=GEMINI_OUTPUT_DIM,\n",
    "    fourier_dim=fourier_dim,\n",
    "    proj_dim=PROJ_DIM,\n",
    "    loc_out_dim=LOC_OUT_DIM,\n",
    "    use_location=False,\n",
    ").to(device)\n",
    "\n",
    "head_text_loc = GeminiHead(\n",
    "    text_dim=GEMINI_OUTPUT_DIM,\n",
    "    fourier_dim=fourier_dim,\n",
    "    proj_dim=PROJ_DIM,\n",
    "    loc_out_dim=LOC_OUT_DIM,\n",
    "    use_location=True,\n",
    ").to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53o3JsLAfayi"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def multiple_negatives_ranking_loss(\n",
    "    anchor_emb: torch.Tensor,\n",
    "    pos_emb: torch.Tensor,\n",
    "    temperature: float = 1.0,\n",
    ") -> torch.Tensor: # nos toca rehacer la función pues no podemos utilizar la de S-BERT\n",
    "    scores = torch.matmul(anchor_emb, pos_emb.T) / temperature\n",
    "    labels = torch.arange(anchor_emb.size(0), device=anchor_emb.device)\n",
    "    return F.cross_entropy(scores, labels)\n",
    "\n",
    "\n",
    "triplet_criterion = nn.TripletMarginLoss(margin=0.2, p=2)\n",
    "lambda_triplet = 1.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8474,
     "status": "ok",
     "timestamp": 1764951860483,
     "user": {
      "displayName": "Juan José Corredor Ojeda",
      "userId": "03283161025982388119"
     },
     "user_tz": 300
    },
    "id": "FqH49ec8fd_i",
    "outputId": "5fe87a41-d946-48c4-e30c-467d75cc9ba0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training head: Gemini + Projection (NO location)...\n",
      "[Gemini+Proj(no-loc)] Epoch 1 | L_total=6.4108 | L_MNR=6.2140 | L_triplet=0.1967\n",
      "[Gemini+Proj(no-loc)] Epoch 2 | L_total=6.4051 | L_MNR=6.2132 | L_triplet=0.1919\n",
      "\n",
      "Training head: Gemini + LocHead + Projection (WITH location)...\n",
      "[Gemini+Loc+Proj] Epoch 1 | L_total=6.4115 | L_MNR=6.2139 | L_triplet=0.1976\n",
      "[Gemini+Loc+Proj] Epoch 2 | L_total=6.4065 | L_MNR=6.2129 | L_triplet=0.1936\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_head(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    n_epochs: int = N_EPOCHS,\n",
    "    lr: float = LR,\n",
    "    use_location: bool = True,\n",
    "    name: str = \"head\",\n",
    "):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        weight_decay=0.01,\n",
    "    )\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_mnr = 0.0\n",
    "        total_triplet = 0.0\n",
    "        total_samples = 0\n",
    "\n",
    "        for (\n",
    "            a_text,\n",
    "            p_text,\n",
    "            n_text,\n",
    "            a_four,\n",
    "            p_four,\n",
    "            n_four,\n",
    "        ) in dataloader:\n",
    "            a_text = a_text.to(device)\n",
    "            p_text = p_text.to(device)\n",
    "            n_text = n_text.to(device)\n",
    "\n",
    "            if use_location:\n",
    "                a_four = a_four.to(device)\n",
    "                p_four = p_four.to(device)\n",
    "                n_four = n_four.to(device)\n",
    "            else:\n",
    "                a_four = p_four = n_four = None\n",
    "\n",
    "            anchor_emb = model(a_text, a_four)\n",
    "            pos_emb    = model(p_text, p_four)\n",
    "            neg_emb    = model(n_text, n_four)\n",
    "\n",
    "            loss_mnr = multiple_negatives_ranking_loss(anchor_emb, pos_emb, temperature=1.0)\n",
    "            loss_triplet = triplet_criterion(anchor_emb, pos_emb, neg_emb)\n",
    "            loss = loss_mnr  #+ lambda_triplet * loss_triplet xambiamos este loss, para dejar el mismo que en los otros modelos. \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            bs = anchor_emb.size(0)\n",
    "            total_loss    += loss.item()         * bs\n",
    "            total_mnr     += loss_mnr.item()     * bs\n",
    "            total_triplet += loss_triplet.item() * bs\n",
    "            total_samples += bs\n",
    "\n",
    "        avg_loss    = total_loss    / total_samples\n",
    "        avg_mnr     = total_mnr     / total_samples\n",
    "        avg_triplet = total_triplet / total_samples\n",
    "\n",
    "        print(\n",
    "            f\"[{name}] Epoch {epoch} | \"\n",
    "            f\"L_total={avg_loss:.4f} | \"\n",
    "            f\"L_MNR={avg_mnr:.4f} | \"\n",
    "            f\"L_triplet={avg_triplet:.4f}\"\n",
    "        )\n",
    "\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "print(\"\\nTraining head: Gemini + Projection (NO location)...\")\n",
    "train_head(\n",
    "    head_text_only,\n",
    "    train_loader,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    lr=LR,\n",
    "    use_location=False,\n",
    "    name=\"Gemini+Proj(no-loc)\",\n",
    ")\n",
    "\n",
    "print(\"\\nTraining head: Gemini + LocHead + Projection (WITH location)...\")\n",
    "train_head(\n",
    "    head_text_loc,\n",
    "    train_loader,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    lr=LR,\n",
    "    use_location=True,\n",
    "    name=\"Gemini+Loc+Proj\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def eval_raw_gemini(df: pd.DataFrame) -> dict:\n",
    "\n",
    "    anchors = df[\"anchor_full_text\"].tolist()\n",
    "    positives = df[\"pos_candidate_full_text\"].tolist()\n",
    "    negatives = df[\"neg_candidate_full_text\"].tolist()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        a_emb = torch.from_numpy(encode_gemini(anchors)).to(device)\n",
    "        p_emb = torch.from_numpy(encode_gemini(positives)).to(device)\n",
    "        n_emb = torch.from_numpy(encode_gemini(negatives)).to(device)\n",
    "\n",
    "        a_emb = F.normalize(a_emb, p=2, dim=-1)\n",
    "        p_emb = F.normalize(p_emb, p=2, dim=-1)\n",
    "        n_emb = F.normalize(n_emb, p=2, dim=-1)\n",
    "\n",
    "        pos_cos = 1 - F.cosine_similarity(a_emb, p_emb)\n",
    "        neg_cos = 1 - F.cosine_similarity(a_emb, n_emb)\n",
    "\n",
    "        pos_euc = torch.norm(a_emb - p_emb, p=2, dim=-1)\n",
    "        neg_euc = torch.norm(a_emb - n_emb, p=2, dim=-1)\n",
    "\n",
    "        pos_manh = torch.norm(a_emb - p_emb, p=1, dim=-1)\n",
    "        neg_manh = torch.norm(a_emb - n_emb, p=1, dim=-1)\n",
    "\n",
    "        total = a_emb.size(0)\n",
    "        acc_cos = (pos_cos < neg_cos).sum().item() / total\n",
    "        acc_euc = (pos_euc < neg_euc).sum().item() / total\n",
    "        acc_manh = (pos_manh < neg_manh).sum().item() / total\n",
    "        acc_max = max(acc_cos, acc_euc, acc_manh)\n",
    "\n",
    "    return {\n",
    "        \"cosine_accuracy\": acc_cos,\n",
    "        \"euclidean_accuracy\": acc_euc,\n",
    "        \"manhattan_accuracy\": acc_manh,\n",
    "        \"max_accuracy\": acc_max,\n",
    "    }\n",
    "\n",
    "\n",
    "def eval_head(\n",
    "    model: nn.Module,\n",
    "    dataset: GeminiFourierTripletDataset,\n",
    "    batch_size: int = BATCH_TRAIN,\n",
    "    use_location: bool = True,\n",
    ") -> dict:\n",
    "\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    all_pos_cos = []\n",
    "    all_neg_cos = []\n",
    "    all_pos_euc = []\n",
    "    all_neg_euc = []\n",
    "    all_pos_manh = []\n",
    "    all_neg_manh = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (\n",
    "            a_text,\n",
    "            p_text,\n",
    "            n_text,\n",
    "            a_four,\n",
    "            p_four,\n",
    "            n_four,\n",
    "        ) in loader:\n",
    "            a_text = a_text.to(device)\n",
    "            p_text = p_text.to(device)\n",
    "            n_text = n_text.to(device)\n",
    "\n",
    "            if use_location:\n",
    "                a_four = a_four.to(device)\n",
    "                p_four = p_four.to(device)\n",
    "                n_four = n_four.to(device)\n",
    "            else:\n",
    "                a_four = p_four = n_four = None\n",
    "\n",
    "            a_emb = model(a_text, a_four)\n",
    "            p_emb = model(p_text, p_four)\n",
    "            n_emb = model(n_text, n_four)\n",
    "\n",
    "            pos_cos = 1 - F.cosine_similarity(a_emb, p_emb)\n",
    "            neg_cos = 1 - F.cosine_similarity(a_emb, n_emb)\n",
    "\n",
    "            pos_euc = torch.norm(a_emb - p_emb, p=2, dim=-1)\n",
    "            neg_euc = torch.norm(a_emb - n_emb, p=2, dim=-1)\n",
    "\n",
    "            pos_manh = torch.norm(a_emb - p_emb, p=1, dim=-1)\n",
    "            neg_manh = torch.norm(a_emb - n_emb, p=1, dim=-1)\n",
    "\n",
    "            all_pos_cos.append(pos_cos.cpu())\n",
    "            all_neg_cos.append(neg_cos.cpu())\n",
    "            all_pos_euc.append(pos_euc.cpu())\n",
    "            all_neg_euc.append(neg_euc.cpu())\n",
    "            all_pos_manh.append(pos_manh.cpu())\n",
    "            all_neg_manh.append(neg_manh.cpu())\n",
    "\n",
    "    pos_cos = torch.cat(all_pos_cos)\n",
    "    neg_cos = torch.cat(all_neg_cos)\n",
    "    pos_euc = torch.cat(all_pos_euc)\n",
    "    neg_euc = torch.cat(all_neg_euc)\n",
    "    pos_manh = torch.cat(all_pos_manh)\n",
    "    neg_manh = torch.cat(all_neg_manh)\n",
    "\n",
    "    total = pos_cos.size(0)\n",
    "    acc_cos = (pos_cos < neg_cos).sum().item() / total\n",
    "    acc_euc = (pos_euc < neg_euc).sum().item() / total\n",
    "    acc_manh = (pos_manh < neg_manh).sum().item() / total\n",
    "    acc_max = max(acc_cos, acc_euc, acc_manh)\n",
    "\n",
    "    return {\n",
    "        \"cosine_accuracy\": acc_cos,\n",
    "        \"euclidean_accuracy\": acc_euc,\n",
    "        \"manhattan_accuracy\": acc_manh,\n",
    "        \"max_accuracy\": acc_max,\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 461968,
     "status": "ok",
     "timestamp": 1764952325540,
     "user": {
      "displayName": "Juan José Corredor Ojeda",
      "userId": "03283161025982388119"
     },
     "user_tz": 300
    },
    "id": "UYobbUivfjCI",
    "outputId": "6986638c-076f-4e22-911b-eb098b83c6f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating on val ===\n",
      "[Gemini] Rate limit reached, sleeping 0.5s...\n",
      "[Gemini] Rate limit reached, sleeping 0.5s...\n",
      "[Gemini] Rate limit reached, sleeping 0.5s...\n",
      "[Gemini] Rate limit reached, sleeping 0.5s...\n",
      "[Gemini] Rate limit reached, sleeping 0.5s...\n",
      "Raw Gemini (no head, no loc): {'cosine_accuracy': 0.584, 'euclidean_accuracy': 0.584, 'manhattan_accuracy': 0.5815, 'max_accuracy': 0.584}\n",
      "Gemini + Projection (NO loc): {'cosine_accuracy': 0.6005, 'euclidean_accuracy': 0.6005, 'manhattan_accuracy': 0.6045, 'max_accuracy': 0.6045}\n",
      "Gemini + LocHead + Projection (WITH loc): {'cosine_accuracy': 0.5885, 'euclidean_accuracy': 0.5885, 'manhattan_accuracy': 0.6015, 'max_accuracy': 0.6015}\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Evaluating on val ===\")\n",
    "metrics_raw = eval_raw_gemini(val)\n",
    "print(\"Raw Gemini (no head, no loc):\", metrics_raw)\n",
    "\n",
    "metrics_text_only = eval_head(\n",
    "    head_text_only,\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_TRAIN,\n",
    "    use_location=False,\n",
    ")\n",
    "print(\"Gemini + Projection (NO loc):\", metrics_text_only)\n",
    "\n",
    "metrics_text_loc = eval_head(\n",
    "    head_text_loc,\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_TRAIN,\n",
    "    use_location=True,\n",
    ")\n",
    "print(\"Gemini + LocHead + Projection (WITH loc):\", metrics_text_loc)\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNYHWsqejh0kH87nildCfvS",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
