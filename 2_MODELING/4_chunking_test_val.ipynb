{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "337e038a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.env.prod\")\n",
    "import os\n",
    "from utils.chunker import chunker, chunk_single_text\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a656bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['candidate_id', 'vacant_id', 't_apply', 'stage_max', 'publish_date',\n",
       "       'label', 'scenario', 'vacant_city_loc', 'vacant_full_text',\n",
       "       'vacant_city_ids', 'candidate_full_text', 'candidate_city_loc',\n",
       "       'candidate_city_id', 'candidate_fourier_features',\n",
       "       'no_valid_vacant_city_ids', 'selected_city_id', 'selected_distance',\n",
       "       'exact_match', 'vacant_fourier_feature'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = pd.read_parquet(\"../files/processed/final_datasets/test.parquet\")\n",
    "ds.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a204a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(os.getenv(\"model_name\"), use_fast=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e8f69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vacants = (\n",
    "    ds[[\"vacant_id\", \"vacant_full_text\"]]\n",
    "    .drop_duplicates(\"vacant_id\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "vacant_chunks = vacants[\"vacant_full_text\"].apply(\n",
    "    lambda txt: chunk_single_text(\n",
    "        text=txt,\n",
    "        name=\"job\",          # or \"vacant\" if you prefer\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    ").apply(pd.Series)\n",
    "vacant_chunks[\"job_chunks_input_ids\"] = vacant_chunks[\"job_chunks_input_ids\"].map(\n",
    "    lambda x: json.dumps(x, ensure_ascii=False)\n",
    ")\n",
    "vacant_chunks[\"job_chunks_attention_mask\"] = vacant_chunks[\"job_chunks_attention_mask\"].map(\n",
    "    lambda x: json.dumps(x, ensure_ascii=False)\n",
    ")\n",
    "\n",
    "vacants_chunked = pd.concat(\n",
    "    [vacants[[\"vacant_id\"]], vacant_chunks],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "cands = (\n",
    "    ds[[\"candidate_id\", \"candidate_full_text\"]]\n",
    "    .drop_duplicates(\"candidate_id\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "cand_chunks = cands[\"candidate_full_text\"].apply(\n",
    "    lambda txt: chunk_single_text(\n",
    "        text=txt,\n",
    "        name=\"cand\",\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    ").apply(pd.Series)\n",
    "cand_chunks[\"cand_chunks_input_ids\"] = cand_chunks[\"cand_chunks_input_ids\"].map(\n",
    "    lambda x: json.dumps(x, ensure_ascii=False)\n",
    ")\n",
    "cand_chunks[\"cand_chunks_attention_mask\"] = cand_chunks[\"cand_chunks_attention_mask\"].map(\n",
    "    lambda x: json.dumps(x, ensure_ascii=False)\n",
    ")\n",
    "\n",
    "cands_chunked = pd.concat(\n",
    "    [cands[[\"candidate_id\"]], cand_chunks],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "961f4518",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = (\n",
    "    ds\n",
    "    .merge(vacants_chunked, on=\"vacant_id\", how=\"left\")\n",
    "    .merge(cands_chunked, on=\"candidate_id\", how=\"left\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76ebd6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds[['candidate_id', 'vacant_id', 't_apply', 'stage_max', 'publish_date',\n",
    "       'label', 'vacant_full_text', 'vacant_city_ids',\n",
    "       'candidate_full_text', 'candidate_city_id',\n",
    "       'candidate_fourier_features', 'no_valid_vacant_city_ids',\n",
    "       'selected_city_id', 'selected_distance', 'exact_match',\n",
    "       'vacant_fourier_feature', 'job_chunks_input_ids',\n",
    "       'job_chunks_attention_mask', 'cand_chunks_input_ids',\n",
    "       'cand_chunks_attention_mask']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a02c19b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"vacant_city_ids\"] = ds[\"vacant_city_ids\"].map(\n",
    "    lambda x: json.dumps(\n",
    "        x.tolist() if isinstance(x, np.ndarray) else x,\n",
    "        ensure_ascii=False\n",
    "    )\n",
    ")\n",
    "ds[\"vacant_fourier_feature\"] = ds[\"vacant_fourier_feature\"].map(\n",
    "    lambda x: json.dumps(\n",
    "        x.tolist() if isinstance(x, np.ndarray) else x,\n",
    "        ensure_ascii=False\n",
    "    )\n",
    ")\n",
    "ds[\"candidate_fourier_features\"] = ds[\"candidate_fourier_features\"].map(\n",
    "    lambda x: json.dumps(\n",
    "        x.tolist() if isinstance(x, np.ndarray) else x,\n",
    "        ensure_ascii=False\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e29a41a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ahora sí, escribir parquet\n",
    "ds.to_parquet(\n",
    "    \"../files/processed/final_datasets/test.parquet\",\n",
    "    engine=\"fastparquet\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95ace772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.env.prod\")\n",
    "import os\n",
    "from utils.chunker import chunker, chunk_single_text\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "ds = pd.read_parquet(\"../files/processed/final_datasets/val.parquet\")\n",
    "ds.columns\n",
    "tokenizer = AutoTokenizer.from_pretrained(os.getenv(\"model_name\"), use_fast=False)\n",
    "\n",
    "vacants = (\n",
    "    ds[[\"vacant_id\", \"vacant_full_text\"]]\n",
    "    .drop_duplicates(\"vacant_id\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "vacant_chunks = vacants[\"vacant_full_text\"].apply(\n",
    "    lambda txt: chunk_single_text(\n",
    "        text=txt,\n",
    "        name=\"job\",          # or \"vacant\" if you prefer\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    ").apply(pd.Series)\n",
    "vacant_chunks[\"job_chunks_input_ids\"] = vacant_chunks[\"job_chunks_input_ids\"].map(\n",
    "    lambda x: json.dumps(x, ensure_ascii=False)\n",
    ")\n",
    "vacant_chunks[\"job_chunks_attention_mask\"] = vacant_chunks[\"job_chunks_attention_mask\"].map(\n",
    "    lambda x: json.dumps(x, ensure_ascii=False)\n",
    ")\n",
    "\n",
    "vacants_chunked = pd.concat(\n",
    "    [vacants[[\"vacant_id\"]], vacant_chunks],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# --- 2.2 Unique candidates ---\n",
    "cands = (\n",
    "    ds[[\"candidate_id\", \"candidate_full_text\"]]\n",
    "    .drop_duplicates(\"candidate_id\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "cand_chunks = cands[\"candidate_full_text\"].apply(\n",
    "    lambda txt: chunk_single_text(\n",
    "        text=txt,\n",
    "        name=\"cand\",\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    ").apply(pd.Series)\n",
    "cand_chunks[\"cand_chunks_input_ids\"] = cand_chunks[\"cand_chunks_input_ids\"].map(\n",
    "    lambda x: json.dumps(x, ensure_ascii=False)\n",
    ")\n",
    "cand_chunks[\"cand_chunks_attention_mask\"] = cand_chunks[\"cand_chunks_attention_mask\"].map(\n",
    "    lambda x: json.dumps(x, ensure_ascii=False)\n",
    ")\n",
    "\n",
    "cands_chunked = pd.concat(\n",
    "    [cands[[\"candidate_id\"]], cand_chunks],\n",
    "    axis=1\n",
    ")\n",
    "ds = (\n",
    "    ds\n",
    "    .merge(vacants_chunked, on=\"vacant_id\", how=\"left\")\n",
    "    .merge(cands_chunked, on=\"candidate_id\", how=\"left\")\n",
    ")\n",
    "ds = ds[['candidate_id', 'vacant_id', 't_apply', 'stage_max', 'publish_date',\n",
    "       'label', 'vacant_full_text', 'vacant_city_ids',\n",
    "       'candidate_full_text', 'candidate_city_id',\n",
    "       'candidate_fourier_features', 'no_valid_vacant_city_ids',\n",
    "       'selected_city_id', 'selected_distance', 'exact_match',\n",
    "       'vacant_fourier_feature', 'job_chunks_input_ids',\n",
    "       'job_chunks_attention_mask', 'cand_chunks_input_ids',\n",
    "       'cand_chunks_attention_mask']]\n",
    "ds[\"vacant_city_ids\"] = ds[\"vacant_city_ids\"].map(\n",
    "    lambda x: json.dumps(\n",
    "        x.tolist() if isinstance(x, np.ndarray) else x,\n",
    "        ensure_ascii=False\n",
    "    )\n",
    ")\n",
    "ds[\"vacant_fourier_feature\"] = ds[\"vacant_fourier_feature\"].map(\n",
    "    lambda x: json.dumps(\n",
    "        x.tolist() if isinstance(x, np.ndarray) else x,\n",
    "        ensure_ascii=False\n",
    "    )\n",
    ")\n",
    "ds[\"candidate_fourier_features\"] = ds[\"candidate_fourier_features\"].map(\n",
    "    lambda x: json.dumps(\n",
    "        x.tolist() if isinstance(x, np.ndarray) else x,\n",
    "        ensure_ascii=False\n",
    "    )\n",
    ")\n",
    "# ahora sí, escribir parquet\n",
    "ds.to_parquet(\n",
    "    \"../files/processed/final_datasets/val.parquet\",\n",
    "    engine=\"fastparquet\",\n",
    "    index=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
