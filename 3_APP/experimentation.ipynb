{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac26af9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080893d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.utils.encode_latlon import GeoFourierEncoder\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746a0871",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "MODEL_PATH = os.path.join(BASE_DIR, \"model\", \"nn.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca2fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 8\n",
    "scales_km = (1200, 400, 150, 50)\n",
    "seed = 0\n",
    "\n",
    "enc = GeoFourierEncoder(D=D, scales_km=scales_km, seed=seed)\n",
    "enc.B = np.load(\"./backend/utils/geo_B.npy\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab542a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = 4.641518\n",
    "lon = -74.062047\n",
    "user_feat = enc.transform([[lat, lon]])\n",
    "user_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c115f0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.utils.chunker import chunker\n",
    "from sqlalchemy import Column, BigInteger, Text, Numeric, Boolean, Float\n",
    "from sqlalchemy.dialects.postgresql import ARRAY\n",
    "from transformers import AutoTokenizer\n",
    "import os \n",
    "\n",
    "class VacancyDB(Base):\n",
    "    __tablename__ = \"vacancies\"\n",
    "\n",
    "    id = Column(BigInteger, primary_key=True, index=True)\n",
    "    title = Column(Text, nullable=False)\n",
    "    description = Column(Text, nullable=False)\n",
    "    salary = Column(Numeric(12, 2))\n",
    "    skills = Column(ARRAY(Text))\n",
    "    sectors = Column(ARRAY(Text))\n",
    "    lat = Column(Float)\n",
    "    lon = Column(Float)\n",
    "    remote = Column(Boolean, default=False)\n",
    "    embedding = Column(ARRAY(Float))  # embedding de la vacante\n",
    "class CandidateDB(Base):\n",
    "    __tablename__ = \"candidates\"\n",
    "\n",
    "    id = Column(BigInteger, primary_key=True, index=True)\n",
    "    title = Column(Text, nullable=False)         # rol del candidato\n",
    "    experiences = Column(Text, nullable=False)   # texto de experiencia/CV\n",
    "    salary = Column(Numeric(12, 2))\n",
    "    skills = Column(ARRAY(Text))\n",
    "    sectors = Column(ARRAY(Text))\n",
    "    lat = Column(Float)\n",
    "    lon = Column(Float)\n",
    "    remote = Column(Boolean, default=False)\n",
    "    embedding = Column(ARRAY(Float))  # embedding del candidato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9bcc10",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VacancyDB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild_fourier\u001b[39m(\u001b[38;5;28mobject\u001b[39m:  \u001b[43mVacancyDB\u001b[49m \u001b[38;5;241m|\u001b[39m CandidateDB):\n\u001b[0;32m      2\u001b[0m      lat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39mlat\n\u001b[0;32m      3\u001b[0m      lon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39mlon\n",
      "\u001b[1;31mNameError\u001b[0m: name 'VacancyDB' is not defined"
     ]
    }
   ],
   "source": [
    "def build_fourier(object:  VacancyDB | CandidateDB):\n",
    "     lat = object.lat\n",
    "     lon = object.lon\n",
    "     D = 8\n",
    "     scales_km = (1200, 400, 150, 50)\n",
    "     seed = 0\n",
    "\n",
    "     enc = GeoFourierEncoder(D=D, scales_km=scales_km, seed=seed)\n",
    "     enc.B = np.load(\"./backend/utils/geo_B.npy\") \n",
    "     enc_loc = enc.transform([[lat, lon]])\n",
    "     return enc_loc\n",
    "\n",
    "\n",
    "def coalesce_list(val):\n",
    "    if isinstance(val, list):\n",
    "        return [str(x) for x in val if x not in (None, \"\", float(\"nan\"))]\n",
    "    if pd.isna(val):\n",
    "        return []\n",
    "    return [str(val)]\n",
    "\n",
    "def format_section(label, values):\n",
    "    values = coalesce_list(values)\n",
    "    return f\"{label}: \" + \", \".join(values) if values else \"\"\n",
    "\n",
    "def build_text_candidate(candidate:CandidateDB):\n",
    "    columnas = {'experience_descriptions':\" experiencia\", 'skill_names':\" habilidades\", 'sector_names':\" sectores\"}\n",
    "    candidate[\"full_text\"] = candidate[\"candidate_description\"]\n",
    "    for col in columnas: \n",
    "        col_ = candidate[col].apply(lambda x: format_section(columnas[col], x))\n",
    "        candidate[\"full_text\"] += col_\n",
    "    candidate[\"full_text\"] = candidate[\"full_text\"] + candidate[\"candidate_salary\"].apply(lambda x: \" salario: \" + str(x))\n",
    "    return candidate\n",
    "\n",
    "def build_vacant_text(vacancy:VacancyDB):\n",
    "        columnas = {'skill_names':\" habilidades\", 'sector_names':\" sectores\"}\n",
    "\n",
    "        for col in columnas:\n",
    "              vacant_text[col] = vacant_text[col].fillna(\"\") \n",
    "              print(col)\n",
    "              col_ = vacant_text[col].apply(lambda x: format_section(columnas[col], x) if len(x)>0 else \"\")\n",
    "              vacant_text[\"full_text\"] += col_\n",
    "        vacant_text[\"full_text\"] = vacant_text[\"full_text\"] + vacant_text[\"min_salary\"].apply(lambda x: \" salario: \" + str(x)) \n",
    "        vacant_text= vacant_text[\"full_text\"]\n",
    "\n",
    "\n",
    "        return vacant_text\n",
    "\n",
    "def create_fourier_vacants(features,remote:int):\n",
    "     features = np.concat(features, remote)\n",
    "     return features\n",
    "def load_model() -> SiameseTwoTower:\n",
    "     global _model\n",
    "     if _model is None:\n",
    "         if not os.path.exists(MODEL_PATH):\n",
    "             raise RuntimeError(f\"Modelo nn.pt no encontrado en {MODEL_PATH}\")\n",
    "         m = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "         m.eval()\n",
    "         m.to(DEVICE)\n",
    "         _model = m\n",
    "     return _model\n",
    "\n",
    "def compute_affinity(candidate:CandidateDB, vacancy:VacancyDB):\n",
    "     job_text = build_vacant_text(vacancy)\n",
    "     cand_text = build_text_candidate(candidate)\n",
    "     texts = {\"job\": job_text, \"cand\": cand_text}\n",
    "     tokenizer = AutoTokenizer.from_pretrained(os.getenv(\"model_name\"), use_fast=False)\n",
    "\n",
    "     chunks = chunker(texts, tokenizer)\n",
    "     vac_fou = build_fourier(vacancy)\n",
    "     cand_fou = build_fourier(candidate)\n",
    "     vac_fou = create_fourier_vacants(vac_fou, vacancy.remote)\n",
    "\n",
    "     model = load_model( )\n",
    "     job_input_ids = chunks[\"job_input_ids\"].unsqueeze(1)\n",
    "     job_attention_mask = chunks[\"job_attention_mask\"].unsqueeze(1)\n",
    "     cand_input_ids = chunks[\"cand_input_ids\"].unsqueeze(1)\n",
    "     cand_attention_mask = chunks[\"cand_attention_mask\"].unsqueeze(1)\n",
    "     batch = {\n",
    "        \"job_input_ids\": job_input_ids.to(DEVICE),\n",
    "        \"job_attention_mask\": job_attention_mask.to(DEVICE),\n",
    "        \"cand_input_ids\": cand_input_ids.to(DEVICE),\n",
    "        \"cand_attention_mask\": cand_attention_mask.to(DEVICE),\n",
    "        \"vac_loc_fourier\": vac_fou.to(DEVICE),\n",
    "        \"cand_loc_fourier\": cand_fou.to(DEVICE),\n",
    "    }\n",
    "\n",
    "     with torch.no_grad():\n",
    "         z_job, z_cand, logit_scale = model(batch)\n",
    "         logits = (z_job * z_cand).sum(dim=-1) * logit_scale\n",
    "         prob = torch.sigmoid(logits)[0].item()\n",
    "\n",
    "     return float(prob) \n",
    "\n",
    "     \n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7237b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b06485d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to connect: could not translate host name \"db.mloiyszhesiikueylruv.supabase.co\" to address: Name or service not known\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "DATABASE_URL = \"postgresql://postgres:Sn8NUhuMFVHRB2wN@db.mloiyszhesiikueylruv.supabase.co:5432/postgres\"\n",
    "\n",
    "# Fetch variables\n",
    "USER = \"postgres\"\n",
    "PASSWORD = \"Sn8NUhuMFVHRB2wN\"\n",
    "HOST = \"db.mloiyszhesiikueylruv.supabase.co\"\n",
    "PORT = \"5432\"\n",
    "DBNAME = \"postgres\"\n",
    "\n",
    "# Connect to the database\n",
    "try:\n",
    "    connection = psycopg2.connect(\n",
    "        user=USER,\n",
    "        password=PASSWORD,\n",
    "        host=HOST,\n",
    "        port=PORT,\n",
    "        dbname=DBNAME\n",
    "    )\n",
    "    print(\"Connection successful!\")\n",
    "    \n",
    "    # Create a cursor to execute SQL queries\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    # Example query\n",
    "    cursor.execute(\"SELECT NOW();\")\n",
    "    result = cursor.fetchone()\n",
    "    print(\"Current Time:\", result)\n",
    "\n",
    "    # Close the cursor and connection\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    print(\"Connection closed.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04007d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current time: (datetime.datetime(2025, 12, 11, 0, 51, 46, 942943, tzinfo=datetime.timezone.utc),)\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "DATABASE_URL = \"postgresql://postgres:Sn8NUhuMFVHRB2wN@db.mloiyszhesiikueylruv.supabase.co:5432/postgres\"\n",
    "DATABASE_URL=\"postgresql://postgres.mloiyszhesiikueylruv:Sn8NUhuMFVHRB2wN@aws-0-us-west-2.pooler.supabase.com:6543/postgres\"\n",
    "try:\n",
    "    conn = psycopg2.connect(DATABASE_URL)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT NOW();\")\n",
    "    print(\"Current time:\", cur.fetchone())\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3bfb48",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 2) (1982287436.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[22], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    password=Sn8N\"UhuMFVHRB2wN\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 2)\n"
     ]
    }
   ],
   "source": [
    "user=\"postgres\"\n",
    "password=\"Sn8NUhuMFVHRB2wN\"\n",
    "host=\"db.mloiyszhesiikueylruv.supabase.co\"\n",
    "port=5432\n",
    "dbname=\"postgres\"\n",
    "ping db.mloiyszhesiikueylruv.supabase.co\n",
    "nslookup db.mloiyszhesiikueylruv.supabase.co\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9b0b9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using REPO_ROOT: c:\\Users\\JuanJoseCorredor\\OneDrive - PSYCONOMETRICS SAS\\Documentos\\uniandes\\Tesis 2\\3_APP\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "could not translate host name \"db.mloiyszhesiikueylruv.supabase.co\" to address: Name or service not known\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing REPO_ROOT:\u001b[39m\u001b[38;5;124m\"\u001b[39m, REPO_ROOT)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# --- 3) Simple queries against the real database ---\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mcursor() \u001b[38;5;28;01mas\u001b[39;00m cur:\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;66;03m# Current DB info\u001b[39;00m\n\u001b[0;32m     21\u001b[0m         cur\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT current_database() AS db, current_schema() AS schema;\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\JuanJoseCorredor\\OneDrive - PSYCONOMETRICS SAS\\Documentos\\uniandes\\Tesis 2\\3_APP\\backend\\db_connection.py:27\u001b[0m, in \u001b[0;36mget_conn\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_conn\u001b[39m():\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpsycopg2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATABASE_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRealDictCursor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\JuanJoseCorredor\\miniconda3\\envs\\myenv\\lib\\site-packages\\psycopg2\\__init__.py:135\u001b[0m, in \u001b[0;36mconnect\u001b[1;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m     kwasync[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124masync_\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124masync_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    134\u001b[0m dsn \u001b[38;5;241m=\u001b[39m _ext\u001b[38;5;241m.\u001b[39mmake_dsn(dsn, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 135\u001b[0m conn \u001b[38;5;241m=\u001b[39m _connect(dsn, connection_factory\u001b[38;5;241m=\u001b[39mconnection_factory, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwasync)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cursor_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m     conn\u001b[38;5;241m.\u001b[39mcursor_factory \u001b[38;5;241m=\u001b[39m cursor_factory\n",
      "\u001b[1;31mOperationalError\u001b[0m: could not translate host name \"db.mloiyszhesiikueylruv.supabase.co\" to address: Name or service not known\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pprint import pprint\n",
    "\n",
    "# --- 1) Point this to your repo root ---\n",
    "# Example from your error trace, ADJUST if needed:\n",
    "REPO_ROOT = r\"c:\\Users\\JuanJoseCorredor\\OneDrive - PSYCONOMETRICS SAS\\Documentos\\uniandes\\Tesis 2\\3_APP\"\n",
    "\n",
    "if REPO_ROOT not in sys.path:\n",
    "    sys.path.append(REPO_ROOT)\n",
    "\n",
    "# --- 2) Import real get_conn ---\n",
    "from backend.db_connection import get_conn\n",
    "\n",
    "print(\"Using REPO_ROOT:\", REPO_ROOT)\n",
    "\n",
    "# --- 3) Simple queries against the real database ---\n",
    "with get_conn() as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        # Current DB info\n",
    "        cur.execute(\"SELECT current_database() AS db, current_schema() AS schema;\")\n",
    "        info = cur.fetchone()\n",
    "        print(\"DB info:\")\n",
    "        pprint(info)\n",
    "\n",
    "        # Vacancies count\n",
    "        try:\n",
    "            cur.execute(\"SELECT COUNT(*) AS n FROM vacancies;\")\n",
    "            n_vac = cur.fetchone()\n",
    "            print(\"Vacancies count:\", n_vac)\n",
    "        except Exception as e:\n",
    "            print(\"Could not count vacancies:\", e)\n",
    "\n",
    "        # Candidates count\n",
    "        try:\n",
    "            cur.execute(\"SELECT COUNT(*) AS n FROM candidates;\")\n",
    "            n_cand = cur.fetchone()\n",
    "            print(\"Candidates count:\", n_cand)\n",
    "        except Exception as e:\n",
    "            print(\"Could not count candidates:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b998a933",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JuanJoseCorredor\\miniconda3\\envs\\myenv\\lib\\site-packages\\pydantic\\_internal\\_config.py:383: UserWarning: Valid config keys have changed in V2:\n",
      "* 'orm_mode' has been renamed to 'from_attributes'\n",
      "  warnings.warn(message, UserWarning)\n",
      "c:\\Users\\JuanJoseCorredor\\miniconda3\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad03d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from backend.schemas import VacancyOut  # type: ignore\n",
    "from backend.utils.affinity_calc import compute_affinity  # type: ignore\n",
    "\n",
    "# IMPORTANT: this must point to the SAME fine‑tuned SentenceTransformer\n",
    "# you used in training (base_model_path in the checkpoint config)\n",
    "os.environ[\"HYBRID_BASE_MODEL_PATH\"] = r\"C:/Users/JuanJoseCorredor/OneDrive - PSYCONOMETRICS SAS/Documentos/uniandes/Tesis 2/3_APP/model/checkpoint-76104\"\n",
    "\n",
    "def main():\n",
    "    vacancy = VacancyOut(\n",
    "        id=1,\n",
    "        title=\"Data Scientist\",\n",
    "        description=\"Buscamos un data scientist con experiencia en ML y NLP.\",\n",
    "        salary=60000.0,\n",
    "        skills=[\"python\", \"pandas\", \"pytorch\"],\n",
    "        sectors=[\"tecnología\", \"datos\"],\n",
    "        lat=-34.6037,\n",
    "        lon=-58.3816,\n",
    "        remote=True,\n",
    "        embedding=None,\n",
    "    )\n",
    "\n",
    "    cv_text = (\n",
    "        \"Asor de moda\"\n",
    "    )\n",
    "\n",
    "    affinity = compute_affinity(\n",
    "        vacancy,\n",
    "        cv_text,\n",
    "        candidate_lat=-31.6037,\n",
    "        candidate_lon=-58.3816,\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Affinity: {affinity:.4f} ({affinity * 100:.2f}%)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89eb3616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Affinity: 0.3359 (33.59%)\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
